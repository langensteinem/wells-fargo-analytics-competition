{"name":"Wells Fargo Analytics Competition","tagline":"","body":"  \r\n#How did we do all this???\r\n \r\n220,377 posts, and two weeks to figure them all out. These are very intimidating numbers for a team of freshman with zero experience in the data science world. How does one even start something like this? The first step was to outline potential approaches. After several scrapped ideas, the final was to focus on each bank individually. To do this, we found data points containing “banka”, “bankb”, “bankc”, and “bankd”, and categorized them accordingly. Our next step was divide each bank’s posts based on their sentiment, ie. the post’s attitude, positive or negative. To find the sentiments of each data point, we used a list of positive words and a list a negative words. We compared the words in each data point to the words of the lists, and a score was recorded for each post. If a post contained a positive word, its score went up by one, and if a post contained a negative word, its score went down by one. Posts with scores above two were marked as positive, and posts with scores below negative two were marked as negative. \r\nSo, at this point, the data points have been divided into eight groups: a positive and negative group for each of the four banks. However, not every post from the original data set is a part of one of these groups. Any post that does not mention any of the banks or has a middling sentiment was removed. Also, if a post mentioned two or more of the banks, it was placed in multiple groups.\r\nNow, to determine the subject matters of the posts, we looked at each group as a large body of text. Within each of these bodies, we found the most frequently used words. This gave us some overall topics to look at for each group, instead of classifying each post individually.\r\nBelow is an outline of our thought process when approaching this project.\r\n\r\n![](http://imgur.com/k6m1oTR)\r\n#Overall Topics\r\n\r\nWhat were these overall topics though? We found that almost every bank is either succeeding or failing in customer service, ATM's, fraud/ security, or technology/machines. We can prove this based on the data and its relationship to the original posts. By making a word cloud of both the positive and negative words in each post, we can see which words are used more. For example, the word cloud below is of bank b. \r\n\r\n![](http://i.imgur.com/YTHUTFh.jpg)\r\n\r\nThis word cloud makes it clear that these are positive words and that customer service is very good in bank b. Below is a negative word cloud for bank b. Words like money, and fraud are \r\n\r\n![](http://i.imgur.com/zAB8idR.jpg)\r\n\r\n#R Code\r\n\r\ndf = read.table('dataset.txt',sep=\"|\",header=T)\r\ndf$FullText = as.character(df$FullText)\r\n\r\n###Grab just the texts, so you can load them in the Corpus\r\ndf.texts = as.data.frame(df[,ncol(df)])\r\n\r\ncolnames(df.texts) = 'FullText'\r\n\r\n### Remove non-ascii characters\r\ndf.texts.clean = as.data.frame(iconv(df.texts$FullText, \"latin1\", \"ASCII\", sub=\"\"))\r\ncolnames(df.texts.clean) = 'FullText'\r\n\r\ndf$FullText = df.texts.clean$FullText\r\n\r\n### If you want to test on just 1000 records using df.1000 created below\r\nidx.1000 = sample(1:nrow(df),1000)\r\ndf.1000 = df[idx.1000,]\r\n\r\n### Load using the tm library\r\nlibrary(tm) \r\ndocs <- Corpus(DataframeSource(df.texts.clean))   \r\n\r\n### Strip extra whitespace\r\ndocs <- tm_map(docs, stripWhitespace)\r\n\r\n### Creates dataframes based on which banks the data points mention\r\nbankA.idx = which( sapply( df$FullText , function( x ) grepl( \"BankA\" , x ) ) )\r\nbankB.idx = which( sapply( df$FullText , function( x ) grepl( \"BankB\" , x ) ) )\r\nbankC.idx = which( sapply( df$FullText , function( x ) grepl( \"BankC\" , x ) ) )\r\nbankD.idx = which( sapply( df$FullText , function( x ) grepl( \"BankD\" , x ) ) )\r\n\r\ndf.bankA = df[ bankA.idx ,   ]\r\ndf.bankB = df[ bankB.idx ,   ]\r\ndf.bankC = df[ bankC.idx ,   ]\r\ndf.bankD = df[ bankD.idx ,   ]\r\n\r\n#### This turns out to be too slow\r\n### Add the metadata\r\n### This takes a bit to run\r\n### You can add more here\r\n###for (i in 1:nrow(df)) {\r\n###  meta(docs[[i]],\"MediaType\") = df$MediaType[i]\r\n###  meta(docs[[i]],\"Year\") = df$Year[i]\r\n###  if (grepl(\"BankA\",df$FullText[i])) {\r\n###    meta(docs[[i]],\"BankA\") = T\r\n###  } else {\r\n###    meta(docs[[i]],\"BankA\") = F\r\n###  }\r\n###  if (grepl(\"BankB\",df$FullText[i])) {\r\n###    meta(docs[[i]],\"BankB\") = T\r\n###  } else {\r\n###    meta(docs[[i]],\"BankB\") = F\r\n###  }\r\n###}\r\n###bankA.idx <- meta(docs, \"BankA\") == T\r\n\r\nbankA.docs = docs[bankA.idx]\r\nbankB.docs = docs[bankB.idx]\r\nbankC.docs = docs[bankC.idx]\r\nbankD.docs = docs[bankD.idx]\r\n\r\nsummary(docs)\r\n\r\ndocs <- tm_map(docs, removePunctuation) \r\n\r\n### Since we can't find a great package in R, I'm going to use an\r\n### example I found online to build our own\r\n### Based on: http://www.ihub.co.ke/blogs/23216\r\n\r\n### Only need to do once\r\n### Download and upload: http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar\r\n###system('unrar e opinion-lexicon-English.rar')\r\n\r\npos <- scan('positive-words.txt',what='character',comment.char=';')\r\nneg <- scan('negative-words.txt',what='character',comment.char=';')\r\n\r\nscore.sentiment = function(sentences, pos.words, neg.words, .progress='none')\r\n{\r\n  require(plyr)\r\n  require(stringr)\r\n  \r\n  ### we got a vector of sentences. plyr will handle a list\r\n  ### or a vector as an \"l\" for us\r\n  ### we want a simple array (\"a\") of scores back, so we use \r\n  ### \"l\" + \"a\" + \"ply\" = \"laply\":\r\n  scores = laply(sentences, function(sentence, pos.words, neg.words) {\r\n    \r\n    ### clean up sentences with R's regex-driven global substitute, gsub():\r\n    sentence = gsub('[[:punct:]]', '', sentence)\r\n    sentence = gsub('[[:cntrl:]]', '', sentence)\r\n    sentence = gsub('\\\\d+', '', sentence)\r\n    ### and convert to lower case:\r\n    sentence = tolower(sentence)\r\n    \r\n    ### split into words. str_split is in the stringr package\r\n    word.list = str_split(sentence, '\\\\s+')\r\n    ### sometimes a list() is one level of hierarchy too much\r\n    words = unlist(word.list)\r\n    \r\n    ### compare our words to the dictionaries of positive & negative terms\r\n    pos.matches = match(words, pos.words)\r\n    neg.matches = match(words, neg.words)\r\n    \r\n    ### match() returns the position of the matched term or NA\r\n    ### we just want a TRUE/FALSE:\r\n    pos.matches = !is.na(pos.matches)\r\n    neg.matches = !is.na(neg.matches)\r\n    \r\n    ### and conveniently enough, TRUE/FALSE will be treated as 1/0 by sum():\r\n    score = sum(pos.matches) - sum(neg.matches)\r\n    \r\n    return(score)\r\n  }, pos.words, neg.words, .progress=.progress )\r\n  \r\n  scores.df = data.frame(score=scores, text=sentences)\r\n  return(scores.df)\r\n}\r\n\r\nbankA.scores = score.sentiment(df.bankA$FullText, pos, neg, .progress='text')\r\nbankA.scores$very.pos = as.numeric(bankA.scores$score >= 2)\r\nbankA.scores$very.neg = as.numeric(bankA.scores$score <= -2)\r\n\r\n### how many very positives and very negatives\r\nbankA.numpos = sum(bankA.scores$very.pos)\r\nbankA.numneg = sum(bankA.scores$very.neg)\r\n\r\n### global score\r\nbankA.global_score = round( 100 * bankA.numpos / (bankA.numpos + bankA.numneg) )\r\n\r\nbankA.scores$mediatype = df.bankA$MediaType\r\n\r\nbankB.scores = score.sentiment(df.bankB$FullText, pos, neg, .progress='text')\r\nbankB.scores$very.pos = as.numeric(bankB.scores$score >= 2)\r\nbankB.scores$very.neg = as.numeric(bankB.scores$score <= -2)\r\n\r\n### how many very positives and very negatives\r\nbankB.numpos = sum(bankB.scores$very.pos)\r\nbankB.numneg = sum(bankB.scores$very.neg)\r\n\r\n### global score\r\nbankB.global_score = round( 100 * bankB.numpos / (bankB.numpos + bankB.numneg) )\r\n\r\nbankB.scores$mediatype = df.bankB$MediaType\r\n\r\nbankC.scores = score.sentiment(df.bankC$FullText, pos, neg, .progress='text')\r\nbankC.scores$very.pos = as.numeric(bankC.scores$score >= 2)\r\nbankC.scores$very.neg = as.numeric(bankC.scores$score <= -2)\r\n\r\n### how many very positives and very negatives\r\nbankC.numpos = sum(bankC.scores$very.pos)\r\nbankC.numneg = sum(bankC.scores$very.neg)\r\n\r\n### global score\r\nbankC.global_score = round( 100 * bankC.numpos / (bankC.numpos + bankC.numneg) )\r\n\r\nbankC.scores$mediatype = df.bankC$MediaType\r\n\r\nbankD.scores = score.sentiment(df.bankD$FullText, pos, neg, .progress='text')\r\nbankD.scores$very.pos = as.numeric(bankD.scores$score >= 2)\r\nbankD.scores$very.neg = as.numeric(bankD.scores$score <= -2)\r\n\r\n### how many very positives and very negatives\r\nbankD.numpos = sum(bankD.scores$very.pos)\r\nbankD.numneg = sum(bankD.scores$very.neg)\r\n\r\n### global score\r\nbankD.global_score = round( 100 * bankD.numpos / (bankD.numpos + bankD.numneg) )\r\n\r\nbankD.scores$mediatype = df.bankD$MediaType\r\n\r\n### function for finding frequent words\r\nfind.freq.words <- function( bank.scores , sentiment )\r\n{\r\n  if ( sentiment == \"positive\" )\r\n  {\r\n    texts = as.data.frame( bank.scores[ bank.scores$very.pos == 1 , 2 ] )\r\n  } else {\r\n    texts = as.data.frame( bank.scores[ bank.scores$very.neg == 1 , 2 ] )\r\n  }\r\n  \r\n  \r\n  docs <- Corpus( DataframeSource( texts ) )\r\n  \r\n  docs <- tm_map( docs , removePunctuation )\r\n  docs <- tm_map( docs , removeNumbers )\r\n  docs <- tm_map( docs , tolower )\r\n  docs <- tm_map( docs , removeWords , stopwords( \"english\" ) )\r\n  docs <- tm_map( docs , stripWhitespace )\r\n  docs <- tm_map( docs , PlainTextDocument ) \r\n  \r\n  docs <- tm_map(docs, removeWords, c( \"bank\" , \"banka\" , \"bankb\" , \"bankc\" , \"bankd\" , \"name\" , \"twithndl\" , \"twithndlbanka\" , \"twithndlbankb\" , \"twithndlbankc\" , \"twithndlbankd\" , \"internet\" , \"rettwit\" ) )\r\n  \r\n  dtm  <- DocumentTermMatrix( docs )\r\n  freq <- colSums( as.matrix( dtm ) )\r\n  \r\n  return( freq )\r\n}\r\n\r\nbankA.pos.freq = find.freq.words( bankA.scores , \"positive\" )\r\nbankB.pos.freq = find.freq.words( bankB.scores , \"positive\" )\r\nbankC.pos.freq = find.freq.words( bankC.scores , \"positive\" )\r\nbankD.pos.freq = find.freq.words( bankD.scores , \"positive\" )\r\nbankA.neg.freq = find.freq.words( bankA.scores , \"negative\" )\r\nbankB.neg.freq = find.freq.words( bankB.scores , \"negative\" )\r\nbankC.neg.freq = find.freq.words( bankC.scores , \"negative\" )\r\nbankD.neg.freq = find.freq.words( bankD.scores , \"negative\" )\r\n\r\nbankA.pos.ord <- order( bankA.pos.freq )\r\nbankB.pos.ord <- order( bankB.pos.freq )\r\nbankC.pos.ord <- order( bankC.pos.freq )\r\nbankD.pos.ord <- order( bankD.pos.freq )\r\nbankA.neg.ord <- order( bankA.neg.freq )\r\nbankB.neg.ord <- order( bankB.neg.freq )\r\nbankC.neg.ord <- order( bankC.neg.freq )\r\nbankD.neg.ord <- order( bankD.neg.freq )\r\n\r\n### prints frequent words\r\nbankA.pos.freq[ tail( bankA.pos.ord , 100 ) ]\r\nbankB.pos.freq[ tail( bankB.pos.ord , 100 ) ]\r\nbankC.pos.freq[ tail( bankC.pos.ord , 100 ) ]\r\nbankD.pos.freq[ tail( bankD.pos.ord , 100 ) ]\r\nbankA.neg.freq[ tail( bankA.neg.ord , 100 ) ]\r\nbankB.neg.freq[ tail( bankB.neg.ord , 100 ) ]\r\nbankC.neg.freq[ tail( bankC.neg.ord , 100 ) ]\r\nbankD.neg.freq[ tail( bankD.neg.ord , 100 ) ]\r\n\r\n### gererates word clouds\r\nset.seed( 142 )\r\nwordcloud( names( bankA.pos.freq ), bankA.pos.freq , min.freq = 100 )\r\nwordcloud( names( bankA.neg.freq ), bankA.neg.freq , min.freq = 100 )\r\nwordcloud( names( bankB.pos.freq ), bankB.pos.freq , min.freq = 100 )\r\nwordcloud( names( bankB.neg.freq ), bankB.neg.freq , min.freq = 100 )\r\nwordcloud( names( bankC.pos.freq ), bankC.pos.freq , min.freq = 50 )\r\nwordcloud( names( bankC.neg.freq ), bankC.neg.freq , min.freq = 50 )\r\nwordcloud( names( bankD.pos.freq ), bankD.pos.freq , min.freq = 100 )\r\nwordcloud( names( bankD.neg.freq ), bankD.neg.freq , min.freq = 100 )\r\n\r\n#What now?\r\n\r\n##Customer Service:\r\nIn a J.D. Power U.S. Retail Banking Satisfaction study the top reason for switching banks in 2014 was poor customer service. Customers either want to be able to solve their problems online/on their phone, or they want more personalized service. A good way to solve both of these problems is to have a clear, easy to use, online banking system with the ability to talk to someone over the phone or teach in the bank. Once the online banking is established, less people will use the in bank services and thus those who still use the in bank services will receive more personal attention. Furthermore, according to the American Express 2014 Global Customer Service Barometer, 86% of surveyed customers are satisfied when the employer gives them a correct answer. A very simple way to improve customer satisfaction is to simply train the employees better. Furthermore, by having each employer specialize in certain parts of the bank (creating an account, problems with unknown fees, etc.) the customer can be steered in the right direction from someone who knows a lot about a little rather than a little about a lot.   \r\n\r\n##Atm's:\r\nMost problems with atms are availability and unavailability to the disabled. First off, many people have to use atms of the wrong bank because their bank does not have one close. This causes them to be charged extra, and of course piss them off. Secondly, those in wheelchairs often times cannot reach certain buttons. Furthermore, people who don't have functioning hands cannot use an atm. If a full bank is not close, the person who is handicapped must go a further distance to a full bank. The cheapest way to fix this is to have a system in online banking that allows a person to use their phone (with voice recognition as well) to essentially get money but then pick up the money at an atm using an online receipt. Through this system someone could use siri to do the entire transaction. Those who do not have functioning hands can ask someone to simply grab their money for them, and those in a wheelchair can reach the money since its low enough to reach from sitting. After receiving the money there would be an online confirmation to assure that everything went smoothly. \r\n\r\n##Faud/Security: \r\nThe largest problem with fraud in banking is online banking hackers. These hackers use employee passwords to transfer customer money. Since finger print technology is ever so popular to the point that we have it on our phones, why don't we use that instead of passwords? By updating this technology hacking will be nearly impossible.  \r\nTechnology/Machines: \r\nThe majority of problems with machines are that they are not meant to go 24-7. A lot of the original technology created for normal transactions, but now there is online banking, shopping, and atms 24-7. This constantly over rides the majority of the IT systems and causes systems to shut down. The only real way to permanently fix this problem is to have a system for each part of the bank. One for Atms, one for online banking, one for online transactions (shopping), and one for regular bank transactions. Although this will be a pain and very costly to start from scratch, it is the only way to better the service in the long run.  \r\n\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}